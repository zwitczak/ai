{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network to classfy MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading MNIST data, transformation and noramalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform and normalize data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "train_data=np.reshape(x_train,[60000,784])/ 255.0\n",
    "train_data = train_data.astype(np.float64)\n",
    "train_label = to_categorical(y_train)\n",
    "\n",
    "test_data=np.reshape(x_test,[10000,784])/ 255.0\n",
    "test_data = test_data.astype(np.float64)\n",
    "test_label=to_categorical(y_test)\n",
    "            \n",
    "print(\"train_data shape=\"+str(np.shape(train_data)))\n",
    "print(\"train_label shape=\"+str(np.shape(train_label)))\n",
    "print(\"test_data shape=\"+str(np.shape(test_data)))\n",
    "print(\"test_label shape=\"+str(np.shape(test_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    Z[Z > 0] = 1\n",
    "    Z[Z <= 0] = 0\n",
    "    return Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    \"\"\"\n",
    "    # Subtract the maximum value for numerical stability\n",
    "    e_x = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(AL):\n",
    "    return AL*(1 - AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers_sizes):\n",
    "        self.layer_sizes = layers_sizes\n",
    "        self.num_layers = len(layers_sizes)\n",
    "        self.weights = {f\"W{i}\": np.random.randn(layers_sizes[i - 1], layers_sizes[i]) for i in range(1, len(layers_sizes))}\n",
    "        self.biases = {f\"b{i}\": np.random.randn(1, layers_sizes[i]) for i in range(1, len(layers_sizes))}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        self.costs = []\n",
    "\n",
    "    def cross_entropy_loss(self, AL, Y):\n",
    "        # Z = np.max(AL, axis=1, keepdims=True)\n",
    "        epsilon=1e-8\n",
    "        AL = np.clip(AL, epsilon, 1 - epsilon)\n",
    "        cost = np.sum( - (Y*np.log(AL) + (1-Y) * np.log(1 - AL)), axis=1)\n",
    "        return cost\n",
    "    \n",
    "    def cross_entropy_back(self, AL, Y):\n",
    "        epsilon=1e-8\n",
    "        AL = np.clip(AL, epsilon, 1 - epsilon)\n",
    "        dEdAL =  - (Y * np.divide(1,AL) + (1 - Y)*(np.divide(1,1-AL)))\n",
    "        return dEdAL\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.cache['A0'] = X\n",
    "        for i in range(self.num_layers - 2):\n",
    "            Z = np.dot(self.cache[f\"A{i}\"], self.weights[f\"W{i + 1}\"]) + self.biases[f\"b{i + 1}\"]\n",
    "            self.cache[f\"Z{i + 1}\"] = Z\n",
    "            A = relu(Z)\n",
    "            self.cache[f\"A{i + 1}\"] = A\n",
    "\n",
    "\n",
    "        Z = np.dot(self.cache[f\"A{i + 1}\"], self.weights[f\"W{i + 2}\"]) + self.biases[f\"b{i + 2}\"]\n",
    "        self.cache[f\"Z{i + 2}\"] = Z\n",
    "        A = softmax(Z)\n",
    "        self.cache[f\"A{i + 2}\"] = A\n",
    "        return A\n",
    "    \n",
    "    def linear_activation_backward(self, dEdA, current_layer, activation_function):\n",
    "        \n",
    "        if activation_function == \"softmax\":\n",
    "            dAdZ = softmax_derivative(self.cache[f\"A{current_layer}\"])\n",
    "        elif activation_function == \"relu\":\n",
    "            dAdZ = relu_derivative(self.cache[f\"Z{current_layer}\"])\n",
    "        else:\n",
    "            print(f\"Error: function {activation_function} is not supported\")\n",
    "\n",
    "        dEdZ = np.multiply(dEdA, dAdZ)\n",
    "        dW = (1 / dEdZ.shape[0] ) * np.dot(dEdZ.T, self.cache[f\"A{current_layer - 1}\"]).T\n",
    "        db = (1 / dEdZ.shape[0] ) * np.sum(dEdZ, axis=0)\n",
    "        dEdA_prev = np.dot(dEdZ, self.weights[f\"W{current_layer}\"].T)\n",
    "        self.grads[f\"dW{current_layer}\"] = dW\n",
    "        self.grads[f\"db{current_layer}\"] = db\n",
    "        self.grads[f\"dEdA{current_layer - 1}\"] = dEdA_prev\n",
    "\n",
    "    def backward_pass(self, AL, Y):\n",
    "        cost = self.cross_entropy_loss(AL=AL, Y=Y)\n",
    "        average_cost = (1/cost.shape[0])*np.sum(cost)\n",
    "        self.costs.append(average_cost)\n",
    "\n",
    "        dEdAL = self.cross_entropy_back(AL, Y) \n",
    "\n",
    "        current_layer = self.num_layers - 1 # 3\n",
    "        self.linear_activation_backward(dEdA=dEdAL, current_layer=current_layer, activation_function=\"softmax\")\n",
    "        for i in reversed(range(current_layer - 1)):\n",
    "            current_layer = i + 1\n",
    "            self.linear_activation_backward(dEdA=self.grads[f\"dEdA{current_layer}\"], current_layer=current_layer, activation_function=\"relu\")\n",
    "            # print(i)\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # print(self.weights['W1'][0])\n",
    "            self.weights[f\"W{i + 1}\"] = self.weights[f\"W{i + 1}\"] - (learning_rate * self.grads[f\"dW{i + 1}\"])\n",
    "            self.biases[f\"b{i + 1}\"] = self.biases[f\"b{i + 1}\"] - (learning_rate * self.grads[f\"db{i + 1}\"])\n",
    "            print(self.weights['W1'][0][0])\n",
    "\n",
    "    def plot_cost_graph(self):\n",
    "        x_value = list(range(1, len(self.costs) + 1))\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('cost')\n",
    "        plt.plot(x_value, self.costs, color='g')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self, X_test, Y_test, metrics=[]):\n",
    "        result = self.forward_pass(X_test)\n",
    "        predictions = result.argmax(axis=0)\n",
    "        expected = Y_test.argmax(axis=0)\n",
    "\n",
    "        if \"accuracy\" in metrics:\n",
    "            accuracy = accuracy_score(expected, predictions)\n",
    "            print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def train(self, X, Y, epochs=100, learning_rate=0.01):\n",
    "\n",
    "        for i in range(1, epochs + 1):\n",
    "            AL = self.forward_pass(X)\n",
    "            self.backward_pass(AL, Y)    \n",
    "            self.update_params(learning_rate) \n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"[epoch {i}]\")\n",
    "                self.test(X, Y, metrics=['accuracy'])\n",
    "                print('cost:', self.costs[i-1])\n",
    "\n",
    "        self.plot_cost_graph()\n",
    "        print('trained...') \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        result = self.forward_pass(X_test)\n",
    "        predictions = result.argmax(axis=0)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN([784, 128, 64, 10])\n",
    "nn.train(train_data, train_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
